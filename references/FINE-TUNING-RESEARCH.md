# Fine-Tuning Model AraÅŸtÄ±rmasÄ±: Viral Sosyal Medya Ä°Ã§erik Ãœretimi

**Tarih**: 11 Åubat 2026
**Proje**: Type Hype / ContentFactory
**AmaÃ§**: Viral tweet, thread, quote Ã¼retimi iÃ§in en uygun fine-tuning stratejisi

---

## 1. Executive Summary

### ğŸ¥‡ DÃ¼ÅŸÃ¼k BÃ¼tÃ§e (Ã–nerilen BaÅŸlangÄ±Ã§): GPT-4.1-nano Fine-tuning
- **Training**: $1.50/1M token | **Inference**: $0.20 input, $0.80 output /1M token
- 500 tweet ile baÅŸla, OpenAI API Ã¼zerinden 10 dakikada fine-tune et
- Toplam baÅŸlangÄ±Ã§ maliyeti: ~$2-5 (ilk fine-tune)
- **Neden**: En dÃ¼ÅŸÃ¼k maliyet, sÄ±fÄ±r infra, anÄ±nda baÅŸla, TÃ¼rkÃ§e kalitesi yeterli

### ğŸ¥ˆ Orta BÃ¼tÃ§e (En Ä°yi DeÄŸer): Qwen3-8B + Unsloth (LoRA) via Together AI
- **Training**: $0.48/1M token (Together AI) veya Ã¼cretsiz (Colab + Unsloth)
- **Inference**: $0.18/1M token (Together AI serverless)
- AÃ§Ä±k kaynak, kendi adapter'Ä±nÄ± eÄŸit, istediÄŸin yerde Ã§alÄ±ÅŸtÄ±r
- **Neden**: MÃ¼kemmel TÃ¼rkÃ§e, yaratÄ±cÄ± yazÄ±da gÃ¼Ã§lÃ¼, esnek hosting

### ğŸ¥‰ YÃ¼ksek BÃ¼tÃ§e (Maksimum Kalite): GPT-4.1-mini Fine-tuning
- **Training**: $5.00/1M token | **Inference**: $0.80 input, $3.20 output /1M token
- Daha bÃ¼yÃ¼k model kapasitesi, daha iyi stil yakalama
- **Neden**: OpenAI ekosistemi, en iyi genel kalite, production-ready

---

## 2. DetaylÄ± Model KarÅŸÄ±laÅŸtÄ±rma Tablosu

### 2.1 API TabanlÄ± (Managed Fine-tuning)

| Model | Boyut | FT YÃ¶ntemi | Training $/1M tok | Inference $/1M tok (in/out) | Min Veri | TÃ¼rkÃ§e (1-5) | YaratÄ±cÄ± (1-5) | Hosting | AÃ§Ä±k Kaynak |
|-------|-------|-----------|-------------------|---------------------------|----------|-------------|----------------|---------|-------------|
| GPT-4.1-nano | ~kÃ¼Ã§Ã¼k | SFT | $1.50 | $0.20 / $0.80 | 10 Ã¶rnek | 3.5 | 3.5 | API | HayÄ±r |
| GPT-4.1-mini | ~orta | SFT | $5.00 | $0.80 / $3.20 | 10 Ã¶rnek | 4 | 4.5 | API | HayÄ±r |
| GPT-4.1 | ~bÃ¼yÃ¼k | SFT | $25.00 | $3.00 / $12.00 | 10 Ã¶rnek | 4.5 | 5 | API | HayÄ±r |
| Mistral Small/7B | 7B | SFT (LoRA) | ~$4 min Ã¼cret | API fiyatÄ± deÄŸiÅŸken | 50+ Ã¶rnek | 3 | 3.5 | API | KÄ±smen |
| Gemini (tuning) | deÄŸiÅŸken | SFT | Ãœcretsiz (sÄ±nÄ±rlÄ±) | Gemini API fiyatlarÄ± | 20+ Ã¶rnek | 4 | 4 | API | HayÄ±r |

### 2.2 AÃ§Ä±k Kaynak + Platform (Together AI / Fireworks)

| Model | Boyut | FT YÃ¶ntemi | Training $/1M tok | Inference $/1M tok (in/out) | Min Veri | TÃ¼rkÃ§e (1-5) | YaratÄ±cÄ± (1-5) | Hosting | Platform |
|-------|-------|-----------|-------------------|---------------------------|----------|-------------|----------------|---------|----------|
| Qwen3-8B | 8B | LoRA/Full | $0.48 (Together LoRA) | $0.18 / $0.18 | 50+ | 4.5 | 4 | Both | Together/Fireworks |
| Qwen3-4B | 4B | LoRA/Full | $0.48 (Together LoRA) | ~$0.10 / $0.10 | 50+ | 4 | 3.5 | Both | Together/Fireworks |
| Qwen3-30B-A3B (MoE) | 30B (3B aktif) | LoRA | $0.48 | $0.15 / $1.50 | 50+ | 5 | 4.5 | Both | Together |
| Llama 3.1-8B | 8B | LoRA/Full | $0.48 / $0.54 | $0.18 / $0.18 | 50+ | 3 | 3.5 | Both | Together/Fireworks |
| Llama 3.2-3B | 3B | LoRA/Full | $0.48 / $0.54 | $0.06 / $0.06 | 50+ | 2.5 | 3 | Both | Together |
| Gemma 3-4B | 4B | LoRA/Full | $0.48 / $0.54 | $0.10 / $0.10 | 50+ | 3.5 | 3.5 | Both | Together/Fireworks |
| Gemma 3-12B | 12B | LoRA/Full | $0.48 | $0.20 / $0.20 | 50+ | 4 | 4 | Both | Together/Fireworks |
| Mistral 7B v0.2 | 7B | LoRA/Full | $0.48 / $0.54 | $0.20 / $0.20 | 50+ | 3 | 3.5 | Both | Together/Fireworks |
| DeepSeek-V3.1 | 671B MoE | LoRA | $10.00 | $0.60 / $1.70 | 100+ | 4.5 | 4.5 | API | Together ($20 min) |
| gpt-oss-20B | 20B | LoRA/Full | $0.48 (16B altÄ±) | $0.05 / $0.20 | 50+ | 3.5 | 4 | Both | Together/Fireworks |

### 2.3 Self-Host (Unsloth + Kendi GPU)

| Model | Boyut | FT YÃ¶ntemi | Training Maliyeti | Inference Maliyeti | Min VRAM | TÃ¼rkÃ§e (1-5) | YaratÄ±cÄ± (1-5) | Not |
|-------|-------|-----------|-------------------|-------------------|----------|-------------|----------------|-----|
| Qwen3-8B | 8B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | Ãœcretsiz (local) | 6GB (4bit) | 4.5 | 4 | Colab T4 yeterli |
| Qwen3-4B | 4B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | Ãœcretsiz (local) | 4GB (4bit) | 4 | 3.5 | En hafif seÃ§enek |
| Llama 3.1-8B | 8B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | Ãœcretsiz (local) | 6GB (4bit) | 3 | 3.5 | Ä°ngilizce gÃ¼Ã§lÃ¼ |
| Gemma 3-4B | 4B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | Ãœcretsiz (local) | 4GB (4bit) | 3.5 | 3.5 | Google kalitesi |
| gpt-oss-20B | 20B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | ~$0.05-0.10/1M | 12GB (4bit) | 3.5 | 4 | Yeni, umut vaat eden |
| Phi-4 | 14B | QLoRA (Unsloth) | Ãœcretsiz (Colab) | Ãœcretsiz (local) | 8GB (4bit) | 3 | 3 | Reasoning gÃ¼Ã§lÃ¼, yaratÄ±cÄ± orta |

---

## 3. Model BazlÄ± Pros/Cons

### GPT-4.1-nano (OpenAI)
**Pros**: En dÃ¼ÅŸÃ¼k API maliyeti, sÄ±fÄ±r infra, hÄ±zlÄ± fine-tune, iyi TÃ¼rkÃ§e desteÄŸi, anÄ±nda production-ready
**Cons**: KapalÄ± kaynak, veri OpenAI'a gider, model kÃ¼Ã§Ã¼k olduÄŸu iÃ§in karmaÅŸÄ±k stilleri kaÃ§Ä±rabilir, vendor lock-in

### GPT-4.1-mini (OpenAI)
**Pros**: Ã‡ok iyi kalite/fiyat oranÄ±, gÃ¼Ã§lÃ¼ yaratÄ±cÄ± yazÄ±, mÃ¼kemmel instruction following
**Cons**: nano'dan 3x pahalÄ± inference, kapalÄ± kaynak, veri gizliliÄŸi endiÅŸesi

### GPT-4.1 (OpenAI)
**Pros**: En yÃ¼ksek kalite, en iyi stil yakalama, 1M context window
**Cons**: Ã‡ok pahalÄ± ($25/1M training, $12/1M output), bÃ¼tÃ§e aÅŸÄ±mÄ± riski yÃ¼ksek

### Qwen3-8B (Alibaba)
**Pros**: MÃ¼kemmel TÃ¼rkÃ§e (Ã§ok dilli eÄŸitim), aÃ§Ä±k kaynak, Apache 2.0, Unsloth ile Ã¼cretsiz eÄŸitim, Together AI'da ucuz inference, thinking/non-thinking mode
**Cons**: Ã‡ince aÄŸÄ±rlÄ±klÄ± pre-training (bazen Ã‡ince sÄ±zÄ±ntÄ± olabilir), self-host gerekebilir

### Qwen3-30B-A3B (MoE)
**Pros**: 30B kapasitesi 3B aktif parametre maliyetiyle, Ã§ok iyi TÃ¼rkÃ§e, inference'da hÄ±zlÄ±
**Cons**: MoE modeller fine-tune'da trickier olabilir, Together AI'da yeni

### Llama 3.1-8B / 3.3-70B (Meta)
**Pros**: Devasa community, tonlarca fine-tune deneyimi, iyi dokÃ¼mantasyon, her platformda desteklenir
**Cons**: TÃ¼rkÃ§e orta seviye (Ä°ngilizce aÄŸÄ±rlÄ±klÄ±), 70B pahalÄ±

### Gemma 3-4B/12B (Google)
**Pros**: KÃ¼Ã§Ã¼k ama kaliteli, Google'Ä±n eÄŸitim verisi kalitesi, iyi Ã§ok dilli destek
**Cons**: Gemma lisansÄ± (bazÄ± kÄ±sÄ±tlamalar), community Llama/Qwen kadar bÃ¼yÃ¼k deÄŸil

### DeepSeek-V3.1
**Pros**: 671B MoE, Ã§ok gÃ¼Ã§lÃ¼, TÃ¼rkÃ§e iyi
**Cons**: Fine-tune Ã§ok pahalÄ± ($10/1M + $20 minimum), Ã‡in veri endiÅŸeleri, bÃ¼yÃ¼k model

### Mistral 7B
**Pros**: Avrupa yapÄ±mÄ±, iyi Ä°ngilizce/FransÄ±zca, hÄ±zlÄ±
**Cons**: TÃ¼rkÃ§e zayÄ±f (Avrupa dilleri odaklÄ±), fine-tune minimum $4

### gpt-oss-20B (OpenAI AÃ§Ä±k Kaynak)
**Pros**: OpenAI kalitesi aÃ§Ä±k kaynak olarak, 20B gÃ¼Ã§lÃ¼ boyut, Unsloth desteÄŸi, Together/Fireworks'te ucuz
**Cons**: Ã‡ok yeni (Åubat 2026), community deneyimi az, lisans detaylarÄ± kontrol edilmeli

### Phi-4 (Microsoft)
**Pros**: Boyutuna gÃ¶re Ã§ok gÃ¼Ã§lÃ¼ reasoning, MIT lisansÄ±
**Cons**: YaratÄ±cÄ± yazÄ±da orta, TÃ¼rkÃ§e zayÄ±f, daha Ã§ok kod/mantÄ±k odaklÄ±

### Gemini Tuning (Google)
**Pros**: Ãœcretsiz fine-tuning (sÄ±nÄ±rlÄ±), iyi TÃ¼rkÃ§e, Google ekosistemi
**Cons**: SÄ±nÄ±rlÄ± kontrol, kapalÄ± kaynak, tuning seÃ§enekleri kÄ±sÄ±tlÄ±, rate limitleri var

---

## 4. Maliyet SenaryolarÄ±

### Hesaplama VarsayÄ±mlarÄ±
- 1 tweet â‰ˆ 150 token (input prompt + output)
- Fine-tune veri seti: her tweet = ~300 token (system + user + assistant mesajlarÄ±)
- Epoch sayÄ±sÄ±: 3 (standart)
- Generation: her tweet Ã¼retimi â‰ˆ 200 token input + 100 token output

### Senaryo A: 1K tweet fine-tune + ayda 10K generation

| Ã‡Ã¶zÃ¼m | Training Maliyeti | AylÄ±k Inference | Toplam Ä°lk Ay | Devam Eden Ay |
|-------|------------------|-----------------|---------------|---------------|
| GPT-4.1-nano | $1.35 (300K tok Ã— 3 epoch Ã— $1.50) | $2.00 input + $0.80 output = $2.80 | $4.15 | $2.80 |
| GPT-4.1-mini | $4.50 | $1.60 + $3.20 = $4.80 | $9.30 | $4.80 |
| Qwen3-8B (Together LoRA) | $0.43 | $0.36 + $0.18 = $0.54 | $0.97 | $0.54 |
| Qwen3-8B (Unsloth + Colab) | $0 | Self-host: $0 (veya Together: $0.54) | $0 - $0.54 | $0 - $0.54 |
| Llama 3.1-8B (Together) | $0.43 | $0.36 + $0.18 = $0.54 | $0.97 | $0.54 |

### Senaryo B: 5K tweet fine-tune + ayda 50K generation

| Ã‡Ã¶zÃ¼m | Training Maliyeti | AylÄ±k Inference | Toplam Ä°lk Ay | Devam Eden Ay |
|-------|------------------|-----------------|---------------|---------------|
| GPT-4.1-nano | $6.75 | $14.00 | $20.75 | $14.00 |
| GPT-4.1-mini | $22.50 | $24.00 | $46.50 | $24.00 |
| Qwen3-8B (Together LoRA) | $2.16 | $2.70 | $4.86 | $2.70 |
| Qwen3-8B (Unsloth + Colab) | $0 | Self-host: ~$0 | ~$0 | ~$0 |

### Senaryo C: 10K tweet fine-tune + ayda 100K generation

| Ã‡Ã¶zÃ¼m | Training Maliyeti | AylÄ±k Inference | Toplam Ä°lk Ay | Devam Eden Ay |
|-------|------------------|-----------------|---------------|---------------|
| GPT-4.1-nano | $13.50 | $28.00 | $41.50 | $28.00 |
| GPT-4.1-mini | $45.00 | $48.00 | $93.00 | $48.00 |
| Qwen3-8B (Together LoRA) | $4.32 | $5.40 | $9.72 | $5.40 |
| Qwen3-8B (Unsloth, Colab Pro) | ~$10/ay Colab | Self-host veya Together | $10-15 | $5-10 |
| gpt-oss-20B (Together) | $4.32 | $1.00 + $2.00 = $3.00 | $7.32 | $3.00 |

---

## 5. Ã–nerilen Strateji (AdÄ±m AdÄ±m)

### Faz 1: HÄ±zlÄ± MVP (Hafta 1-2) â†’ GPT-4.1-nano
1. **Mevcut 500 tweet verisini** JSONL formatÄ±na Ã§evir (system/user/assistant)
2. OpenAI fine-tuning API ile **GPT-4.1-nano** fine-tune et (~$1-2)
3. Kaliteyi deÄŸerlendir: 50 tweet Ã¼ret, manuel olarak puanla
4. Bu baseline olacak, diÄŸer modelleri bununla karÅŸÄ±laÅŸtÄ±r

### Faz 2: AÃ§Ä±k Kaynak Alternatif (Hafta 2-3) â†’ Qwen3-8B + Unsloth
1. Google Colab Pro ($10/ay) al
2. **Unsloth** ile Qwen3-8B'yi QLoRA ile fine-tune et (Ã¼cretsiz notebook'lar var)
3. AynÄ± veri setini kullan, sonuÃ§larÄ± GPT-4.1-nano ile karÅŸÄ±laÅŸtÄ±r
4. Adapter'Ä± kaydet, Together AI veya Fireworks'e yÃ¼kle

### Faz 3: Veri Toplama ve Ä°yileÅŸtirme (Hafta 3-8)
1. Veri setini **500 â†’ 2000+** tweet'e Ã§Ä±kar
2. Veri Ã§eÅŸitliliÄŸini artÄ±r: farklÄ± tonlar, konular, formatlar
3. **DPO (Direct Preference Optimization)** dene: iyi tweet vs kÃ¶tÃ¼ tweet Ã§iftleri
4. Her iki modeli yeniden fine-tune et, karÅŸÄ±laÅŸtÄ±r

### Faz 4: Ã–lÃ§eklendirme (Ay 2+)
1. Kazanan modeli production'a al
2. EÄŸer aÃ§Ä±k kaynak kazandÄ±ysa: Together AI serverless veya kendi GPU
3. EÄŸer GPT-4.1-nano kazandÄ±ysa: batch API ile maliyet %50 dÃ¼ÅŸÃ¼r
4. Veri setini sÃ¼rekli bÃ¼yÃ¼t (10K+ hedefi)
5. A/B testing ile gerÃ§ek engagement verisi topla

### Ã–nemli Ä°puÃ§larÄ±
- **Veri kalitesi > veri miktarÄ±**: 500 mÃ¼kemmel tweet > 5000 vasat tweet
- **System prompt'u fine-tune'a dahil et**: Stil, ton, kiÅŸilik tanÄ±mÄ±nÄ± system message olarak koy
- **Ã‡ift dil stratejisi**: TÃ¼rkÃ§e ve Ä°ngilizce tweetleri ayrÄ± ayrÄ± etiketle, modele dil bilgisi ver
- **Evaluation framework kur**: BLEU/ROUGE yerine, viral metrikler (engagement tahmin skoru) kullan
- **OpenAI Batch API**: Toplu Ã¼retimde %50 indirim, 24 saat iÃ§inde teslim

---

## 6. Platform KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Platform | Fine-tune DesteÄŸi | Inference | Avantaj | Dezavantaj |
|----------|------------------|-----------|---------|-----------|
| **OpenAI API** | GPT-4.1 serisi SFT | Serverless | En kolay, en hÄ±zlÄ±, batch API | KapalÄ± kaynak, vendor lock-in |
| **Together AI** | 100+ model, LoRA/Full/DPO | Serverless + Dedicated | En geniÅŸ model seÃ§imi, ucuz | BazÄ± modellerde minimum Ã¼cret |
| **Fireworks AI** | 80+ model, SFT/DPO | Serverless + On-demand | HÄ±zlÄ± inference, basit fiyatlandÄ±rma | Model seÃ§imi Together'dan az |
| **Unsloth** | TÃ¼m aÃ§Ä±k kaynak modeller | Yok (sadece training) | Ãœcretsiz, 2x hÄ±z, %70 az VRAM | Inference ayrÄ±ca Ã§Ã¶zÃ¼lmeli |
| **Hugging Face AutoTrain** | BirÃ§ok model | Inference Endpoints | Kolay UI, Spaces | Fiyat karÄ±ÅŸÄ±k |
| **Replicate** | SÄ±nÄ±rlÄ± | Serverless | Basit API | Fine-tune desteÄŸi sÄ±nÄ±rlÄ± |
| **Google AI Studio** | Gemini tuning | API | Ãœcretsiz tuning | KÄ±sÄ±tlÄ± kontrol |
| **Mistral Platform** | Mistral modelleri | API | Avrupa veri gÃ¼venliÄŸi | PahalÄ± ($4 min), TÃ¼rkÃ§e zayÄ±f |

---

## 7. TÃ¼rkÃ§e Dil Kalitesi NotlarÄ±

TÃ¼rkÃ§e iÃ§in en iyi sonuÃ§ veren modeller (sÄ±rasÄ±yla):
1. **Qwen3 serisi**: Alibaba'nÄ±n Ã§ok dilli eÄŸitim verisi TÃ¼rkÃ§e'yi iyi kapsÄ±yor
2. **GPT-4.1 serisi**: OpenAI'Ä±n genel kalitesi TÃ¼rkÃ§e'de de iyi yansÄ±yor
3. **Gemma 3**: Google'Ä±n Ã§ok dilli verisi gÃ¼Ã§lÃ¼
4. **DeepSeek V3**: GeniÅŸ veri seti, TÃ¼rkÃ§e dÃ¼zgÃ¼n
5. **Llama 3.x**: Ä°ngilizce aÄŸÄ±rlÄ±klÄ±, TÃ¼rkÃ§e orta
6. **Mistral**: Avrupa dilleri gÃ¼Ã§lÃ¼ ama TÃ¼rkÃ§e zayÄ±f

**Not**: Fine-tuning ile tÃ¼m modellerin TÃ¼rkÃ§e kalitesi Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rÄ±labilir. 500+ TÃ¼rkÃ§e tweet ile eÄŸitim, zayÄ±f modelleri bile kabul edilebilir seviyeye Ã§eker.

---

## 8. SonuÃ§ ve Tavsiye

**BaÅŸlangÄ±Ã§ iÃ§in GPT-4.1-nano + Qwen3-8B ikili stratejisi** Ã¶neriyorum:

1. **GPT-4.1-nano** ile hÄ±zlÄ± baÅŸla (maliyet: $5 altÄ±)
2. **Qwen3-8B + Unsloth** ile paralel dene (maliyet: $0)
3. Hangisi daha iyi tweet Ã¼retiyorsa onu scale et
4. Veri setini bÃ¼yÃ¼ttÃ¼kÃ§e her ikisini de yeniden eÄŸit

Bu yaklaÅŸÄ±m:
- Toplam baÅŸlangÄ±Ã§ maliyeti: **$5 altÄ±**
- Risk: **DÃ¼ÅŸÃ¼k** (iki farklÄ± strateji paralel)
- Production'a geÃ§iÅŸ: **1-2 hafta**
- Ã–lÃ§eklenebilirlik: **YÃ¼ksek** (her iki yolda da)

---

## 9. Kaynaklar

- OpenAI Pricing: https://openai.com/api/pricing/
- Together AI Pricing: https://www.together.ai/pricing
- Together AI Fine-tuning Models: https://docs.together.ai/docs/fine-tuning-models
- Fireworks AI Pricing: https://fireworks.ai/pricing
- Mistral Fine-tuning Docs: https://docs.mistral.ai/capabilities/finetuning/
- Unsloth GitHub: https://github.com/unslothai/unsloth
- Unsloth Docs: https://unsloth.ai/docs
- Google Gemini Tuning: https://ai.google.dev/gemini-api/docs/tuning
- Replicate Pricing: https://replicate.com/pricing

**Not**: Fiyatlar 11 Åubat 2026 itibarÄ±yla gÃ¼nceldir. AI fiyatlarÄ± hÄ±zla dÃ¼ÅŸmektedir, karar vermeden Ã¶nce gÃ¼ncel fiyatlarÄ± kontrol edin.
