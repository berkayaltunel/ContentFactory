"""Magic Morning â€” JIT daily draft generator.

KullanÄ±cÄ± giriÅŸ yaptÄ±ÄŸÄ±nda, bugÃ¼n iÃ§in taslak yoksa:
1. KullanÄ±cÄ±nÄ±n niche'lerine uygun top 3 trendi seÃ§ (semantic dedup)
2. brand_voice DNA ile 3 farklÄ± tweet taslaÄŸÄ± Ã¼ret
3. daily_drafts tablosuna kaydet, cache olarak dÃ¶ndÃ¼r

Maliyet: ~$0.003/kullanÄ±cÄ±/gÃ¼n (sadece giriÅŸ yapanlara)
"""
import json
import logging
import os
from datetime import datetime, timezone, timedelta
from typing import Optional
import uuid

from openai import OpenAI

logger = logging.getLogger(__name__)

openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", ""))


def _get_supabase():
    from server import supabase
    return supabase


def _select_diverse_trends(trends: list, count: int = 3) -> list:
    """Top trendlerden semantic olarak farklÄ± olanlarÄ± seÃ§.
    
    Basit yaklaÅŸÄ±m: keyword overlap ile dedup.
    Cosine similarity yerine keyword jaccard distance kullanÄ±yoruz (zero-cost, embedding gerektirmez).
    """
    if len(trends) <= count:
        return trends

    selected = [trends[0]]  # En yÃ¼ksek skorlu her zaman dahil

    for trend in trends[1:]:
        if len(selected) >= count:
            break

        # Keyword overlap kontrolÃ¼
        t_keywords = set(k.lower() for k in (trend.get("keywords") or []))
        t_topic_words = set(trend.get("topic", "").lower().split())
        t_all = t_keywords | t_topic_words

        is_duplicate = False
        for sel in selected:
            s_keywords = set(k.lower() for k in (sel.get("keywords") or []))
            s_topic_words = set(sel.get("topic", "").lower().split())
            s_all = s_keywords | s_topic_words

            # Jaccard similarity
            if t_all and s_all:
                overlap = len(t_all & s_all) / len(t_all | s_all)
                if overlap > 0.3:  # %30+ overlap = muhtemelen aynÄ± konu
                    is_duplicate = True
                    break

        if not is_duplicate:
            selected.append(trend)

    # Yeterli bulunamadÄ±ysa sÄ±rayla ekle
    if len(selected) < count:
        for trend in trends:
            if trend not in selected:
                selected.append(trend)
            if len(selected) >= count:
                break

    return selected[:count]


async def _generate_evergreen_drafts(user_id: str, niches: list, brand_voice: dict, display_name: str, platform: str) -> dict:
    """Trend yokken evergreen (zamansÄ±z) taslaklar Ã¼ret."""
    sb = _get_supabase()
    tones = brand_voice.get("tones", {})
    active_tones = {k: v for k, v in tones.items() if v > 0}
    tone_labels = {"informative": "Bilgi Verici", "friendly": "Samimi", "witty": "Esprili",
                   "aggressive": "Agresif", "inspirational": "Ä°lham Verici"}
    tone_str = ", ".join(f"%{v} {tone_labels.get(k, k)}" for k, v in sorted(active_tones.items(), key=lambda x: -x[1]))
    niche_str = ", ".join(niches[:5]) if niches else "genel"

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"""BugÃ¼n gÃ¼ndemde Ã¶ne Ã§Ä±kan bir trend yok. KullanÄ±cÄ±nÄ±n niÅŸlerine ve sesine uygun 3 zamansÄ±z (evergreen) tweet taslaÄŸÄ± Ã¼ret.

NiÅŸ alanlarÄ±: {niche_str}
Ton dengesi: {tone_str}

KURALLAR:
- ZamansÄ±z, her zaman geÃ§erli tavsiyeler/dÃ¼ÅŸÃ¼nceler
- KiÅŸisel deneyim veya sektÃ¶rel bilgi odaklÄ±
- Twitter max 280 karakter
- Emoji kullanma, doÄŸal yaz
- Her biri farklÄ± aÃ§Ä±dan

JSON: {{"drafts": [{{"content": "...", "insight": "Neden bu konu her zaman geÃ§erli"}}]}}"""},
                {"role": "user", "content": "3 zamansÄ±z tweet taslaÄŸÄ± Ã¼ret."}
            ],
            temperature=0.8,
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        drafts_data = result.get("drafts", [])
    except Exception as e:
        logger.error(f"Evergreen generation error: {e}")
        return {"drafts": [], "cached": False, "reason": "generation_failed"}

    saved = []
    for draft in drafts_data[:3]:
        doc = {
            "id": str(uuid.uuid4()),
            "user_id": user_id,
            "trend_id": None,
            "content": draft.get("content", ""),
            "platform": platform,
            "status": "pending",
            "trend_topic": "ğŸ’¡ ZamansÄ±z Tavsiye",
            "trend_summary": None,
            "insight": draft.get("insight", ""),
            "created_at": datetime.now(timezone.utc).isoformat(),
        }
        try:
            sb.table("daily_drafts").insert(doc).execute()
            saved.append(doc)
        except Exception as e:
            logger.error(f"Evergreen draft save error: {e}")

    return {"drafts": saved, "cached": False, "evergreen": True}


async def generate_magic_morning(user_id: str, platform: str = "twitter") -> dict:
    """KullanÄ±cÄ± iÃ§in gÃ¼nlÃ¼k taslaklar Ã¼ret (JIT).
    
    Returns:
        {"drafts": [...], "cached": bool}
    """
    sb = _get_supabase()

    # 1. BugÃ¼nkÃ¼ taslaklar var mÄ±? (cache check)
    today_start = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    existing = sb.table("daily_drafts") \
        .select("*") \
        .eq("user_id", user_id) \
        .gte("created_at", today_start.isoformat()) \
        .order("created_at", desc=False) \
        .limit(5) \
        .execute()

    if existing.data and len(existing.data) >= 3:
        return {"drafts": existing.data, "cached": True}

    # 2. KullanÄ±cÄ±nÄ±n profil bilgilerini Ã§ek
    profile = sb.table("user_settings") \
        .select("display_name, niches, brand_voice") \
        .eq("user_id", user_id) \
        .limit(1) \
        .execute()

    niches = []
    brand_voice = {}
    display_name = ""
    if profile.data:
        niches = profile.data[0].get("niches") or []
        brand_voice = profile.data[0].get("brand_voice") or {}
        display_name = profile.data[0].get("display_name") or ""

    # 3. Trendleri Ã§ek (son 48h, score >= 60)
    cutoff = (datetime.now(timezone.utc) - timedelta(hours=48)).isoformat()
    trend_query = sb.table("trends") \
        .select("id, topic, summary, keywords, content_angle, category, score, url") \
        .eq("is_visible", True) \
        .gte("created_at", cutoff) \
        .gte("score", 60) \
        .order("score", desc=True) \
        .limit(20) \
        .execute()

    trends = trend_query.data or []
    if not trends:
        # Evergreen fallback: trend yoksa DNA'ya uygun zamansÄ±z tavsiyeler Ã¼ret
        return await _generate_evergreen_drafts(user_id, niches, brand_voice, display_name, platform)

    # 4. Niche filtering (varsa)
    if niches:
        from routes.trends import NICHE_KEYWORDS
        niche_kws = []
        for niche in niches:
            niche_kws.extend(NICHE_KEYWORDS.get(niche, []))
        niche_kws_lower = [kw.lower() for kw in niche_kws]

        def matches(t):
            t_kws = [k.lower() for k in (t.get("keywords") or [])]
            topic_lower = (t.get("topic") or "").lower()
            return any(nk in tk or tk in nk for nk in niche_kws_lower for tk in t_kws) or \
                   any(nk in topic_lower for nk in niche_kws_lower)

        niche_trends = [t for t in trends if matches(t)]
        # Pad with general if not enough
        if len(niche_trends) < 3:
            niche_trends.extend([t for t in trends if t not in niche_trends])
        trends = niche_trends

    # 5. Semantic dedup â€” farklÄ± 3 trend seÃ§
    selected = _select_diverse_trends(trends, count=3)

    if not selected:
        return {"drafts": [], "cached": False, "reason": "no_matching_trends"}

    # 6. Brand voice context oluÅŸtur
    tones = brand_voice.get("tones", {})
    active_tones = {k: v for k, v in tones.items() if v > 0}
    tone_labels = {"informative": "Bilgi Verici", "friendly": "Samimi", "witty": "Esprili",
                   "aggressive": "Agresif", "inspirational": "Ä°lham Verici"}
    tone_str = ", ".join(f"%{v} {tone_labels.get(k, k)}" for k, v in sorted(active_tones.items(), key=lambda x: -x[1]))

    principles = brand_voice.get("principles", [])
    avoid = brand_voice.get("avoid", [])

    voice_context = f"Ton dengesi: {tone_str}" if tone_str else "Ton: Dengeli"
    if principles:
        voice_context += f"\nÄ°lkeler: {', '.join(principles[:5])}"
    if avoid:
        voice_context += f"\nKaÃ§Ä±nÄ±lacaklar: {', '.join(avoid[:5])}"

    # 7. GPT ile 3 taslak Ã¼ret
    trends_block = ""
    for i, t in enumerate(selected):
        trends_block += f"\n## Trend {i+1}: {t['topic']}\n"
        trends_block += f"Ã–zet: {t.get('summary', '')}\n"
        trends_block += f"AÃ§Ä±: {t.get('content_angle', '')}\n"
        trends_block += f"Kategori: {t.get('category', '')}\n"

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"""Sen bir sosyal medya iÃ§erik asistanÄ±sÄ±n.
KullanÄ±cÄ±nÄ±n marka sesini kullanarak, verilen trendler hakkÄ±nda {platform} iÃ§in 3 farklÄ± iÃ§erik taslaÄŸÄ± Ã¼ret.

KULLANICI SESÄ°:
{voice_context}

KURALLAR:
- Her taslak farklÄ± bir trend hakkÄ±nda olmalÄ±
- KullanÄ±cÄ±nÄ±n ton dengesine UYGUN yaz
- Emoji kullanma (kullanÄ±cÄ± isterse ekler)
- AI template kalÄ±plarÄ± kullanma ("Yapay zeka ile ilgili bÃ¼yÃ¼k haber!" gibi kliÅŸeler YASAK)
- Twitter iÃ§in max 280 karakter, doÄŸal ve kiÅŸisel
- Her taslak iÃ§in kÄ±sa bir "insight" yaz: Bu trend kullanÄ±cÄ±nÄ±n niÅŸi iÃ§in neden Ã¶nemli (1 cÃ¼mle, TÃ¼rkÃ§e)

JSON formatÄ±:
{{
  "drafts": [
    {{
      "trend_index": 0,
      "content": "Tweet taslaÄŸÄ±",
      "insight": "Bu konu senin iÃ§in neden Ã¶nemli"
    }},
    ...
  ]
}}"""},
                {"role": "user", "content": f"GÃ¼nÃ¼n trendleri:{trends_block}"}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        drafts_data = result.get("drafts", [])

    except Exception as e:
        logger.error(f"Magic Morning GPT error: {e}")
        return {"drafts": [], "cached": False, "reason": "generation_failed"}

    # 8. DB'ye kaydet
    saved_drafts = []
    for draft in drafts_data[:3]:
        trend_idx = draft.get("trend_index", 0)
        trend = selected[trend_idx] if trend_idx < len(selected) else selected[0]

        doc = {
            "id": str(uuid.uuid4()),
            "user_id": user_id,
            "trend_id": trend.get("id"),
            "content": draft.get("content", ""),
            "platform": platform,
            "status": "pending",
            "trend_topic": trend.get("topic", ""),
            "trend_summary": trend.get("summary", ""),
            "insight": draft.get("insight", ""),
            "created_at": datetime.now(timezone.utc).isoformat(),
        }

        try:
            sb.table("daily_drafts").insert(doc).execute()
            saved_drafts.append(doc)
        except Exception as e:
            logger.error(f"Draft save error: {e}")

    return {"drafts": saved_drafts, "cached": False}
